# ğŸ§  LearnBot â€” First Principles Learning Chatbot

An AI-powered learning chatbot that breaks **any topic** into its prerequisite building blocks, teaches them bottom-up using first principles, and tests understanding with synthesis questions. Built with a hybrid memory system (keyword + semantic search), SQLite caching, and a stateless FastAPI backend.

![Python](https://img.shields.io/badge/Python-3.10+-blue?logo=python)
![FastAPI](https://img.shields.io/badge/FastAPI-0.115-green?logo=fastapi)
![LLM](https://img.shields.io/badge/LLM-Llama%203.3%2070B-purple)
![FAISS](https://img.shields.io/badge/Search-FAISS%20%2B%20Keyword-orange)

---

## ğŸ“‹ Table of Contents

- [How It Works](#-how-it-works)
- [Architecture Overview](#-architecture-overview)
- [Memory System](#-memory-system-3-file-architecture)
- [Hybrid Search Engine](#-hybrid-search-engine)
- [Caching Layer](#-caching-layer-sqlite)
- [Teaching Pipeline](#-teaching-pipeline)
- [Setup & Installation](#-setup--installation)
- [Configuration](#-configuration)
- [Project Structure](#-project-structure)

---

## ğŸ” How It Works

LearnBot uses a **recall-based learning** approach inspired by first principles thinking:

1. **You ask to learn a topic** (e.g., _"Learn: Neural Networks"_)
2. **The LLM recursively decomposes it** into prerequisite concepts, building a tree
3. **Already-mastered concepts are skipped** (checked via the mastery cache)
4. **Teaching happens bottom-up** â€” starting from the simplest leaf concepts
5. **After each concept, a synthesis question** forces you to _combine_ what you learned
6. **Your answer is validated** for genuine integration, not just keyword matching
7. **If you fail, you get hints** â€” if you fail 3 times, the system explains and moves on
8. **Everything is persisted** in markdown memory files and a SQLite cache

```
User: "Learn: Machine Learning"
                â”‚
                â–¼
    â”Œâ”€ Prerequisite Tree Builder (LLM) â”€â”
    â”‚                                    â”‚
    â”‚   Machine Learning                 â”‚
    â”‚   â”œâ”€ Statistics                    â”‚
    â”‚   â”‚  â”œâ”€ Mean & Median [FACT]       â”‚
    â”‚   â”‚  â””â”€ Standard Deviation         â”‚
    â”‚   â”œâ”€ Linear Algebra                â”‚
    â”‚   â”‚  â”œâ”€ Vectors [FACT]             â”‚
    â”‚   â”‚  â””â”€ Matrix Multiplication      â”‚
    â”‚   â””â”€ Optimization                  â”‚
    â”‚      â””â”€ Gradient Descent           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
    Teaching order (bottom-up):
    1. Mean & Median â†’ explain â†’ quiz
    2. Vectors â†’ explain â†’ quiz
    3. Standard Deviation â†’ explain â†’ quiz
    4. Matrix Multiplication â†’ explain â†’ quiz
    5. Gradient Descent â†’ explain â†’ quiz
    6. Statistics â†’ explain â†’ quiz
    7. Linear Algebra â†’ explain â†’ quiz
    8. Optimization â†’ explain â†’ quiz
    9. Machine Learning â†’ final synthesis
```

---

## ğŸ— Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        FRONTEND                                 â”‚
â”‚  index.html + Tailwind CSS + Vanilla JS                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ chat.js  â”‚ â”‚ tree.js  â”‚ â”‚sessions.js â”‚ â”‚memory-store.js â”‚  â”‚
â”‚  â”‚ messages â”‚ â”‚ renders  â”‚ â”‚ localStorageâ”‚ â”‚ syncs memory   â”‚  â”‚
â”‚  â”‚ & submit â”‚ â”‚ tree UI  â”‚ â”‚ persistenceâ”‚ â”‚ from backend   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚       â”‚  POST /api/chat  {message, session_context}            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BACKEND (FastAPI)                             â”‚
â”‚                                                                 â”‚
â”‚  main.py â”€â”€â”€ Stateless API: receives session_context,           â”‚
â”‚              returns session_update                              â”‚
â”‚                                                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ modules/ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚                                                        â”‚     â”‚
â”‚  â”‚  llm_client.py â”€â”€â”€â”€ NVIDIA API (OpenAI-compatible)     â”‚     â”‚
â”‚  â”‚       â”‚                                                â”‚     â”‚
â”‚  â”‚       â”œâ”€â”€â–º prerequisite.py â”€â”€ Recursive tree builder   â”‚     â”‚
â”‚  â”‚       â”œâ”€â”€â–º explainer.py â”€â”€â”€â”€â”€ First-principles teach   â”‚     â”‚
â”‚  â”‚       â”œâ”€â”€â–º synthesis.py â”€â”€â”€â”€â”€ Synthesis Q generator    â”‚     â”‚
â”‚  â”‚       â””â”€â”€â–º validator.py â”€â”€â”€â”€â”€ Answer checker + hints   â”‚     â”‚
â”‚  â”‚                                                        â”‚     â”‚
â”‚  â”‚  memory_manager.py â”€â”€ Read/write 3 markdown files      â”‚     â”‚
â”‚  â”‚  search.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Hybrid keyword + FAISS search   â”‚     â”‚
â”‚  â”‚  cache.py â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ SQLite persistence layer        â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â”‚                                                                 â”‚
â”‚  config.py â”€â”€ Environment-based configuration                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     DATA LAYER                                  â”‚
â”‚                                                                 â”‚
â”‚  data/memory/                                                   â”‚
â”‚  â”œâ”€â”€ memory.md â”€â”€â”€â”€â”€â”€â”€â”€ Lifetime knowledge graph                â”‚
â”‚  â”œâ”€â”€ daily.md â”€â”€â”€â”€â”€â”€â”€â”€â”€ Today's session log                     â”‚
â”‚  â”œâ”€â”€ conversation.md â”€â”€ Active chat transcript                  â”‚
â”‚  â”œâ”€â”€ cache.db â”€â”€â”€â”€â”€â”€â”€â”€â”€ SQLite (embeddings, trees, mastery)     â”‚
â”‚  â””â”€â”€ state.json â”€â”€â”€â”€â”€â”€â”€ Session state snapshots                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—ƒ Memory System (3-File Architecture)

The memory system uses **three markdown files**, each serving a distinct purpose. This design is inspired by the [OpenClaw memory pattern](https://github.com/openclaw):

### `memory.md` â€” Lifetime Knowledge Graph

Stores **everything the user has ever learned**, persisted across sessions:

```markdown
# User Knowledge Graph

## User Profile
- Learning style: unknown
- Struggles with: unknown

## Topics Mastered

### Linear Algebra
- Mastered: 2026-02-15 14:30
- Synthesis answer: Vectors are like arrows with direction...
- Key insight: Connected matrix ops to image transformations

## Learning Progress Tree
â”œâ”€ Machine Learning
â”‚  â”œâ”€ Statistics
â”‚  â””â”€ Linear Algebra [MASTERED]

## Synthesis Insights
```

**How it's used:**
- When the user starts learning a new topic, mastered concepts are looked up and **skipped** in the teaching order
- The `## Learning Progress Tree` section is updated whenever a new topic tree is built
- Search queries match against this file for long-term context

### `daily.md` â€” Session Log

Tracks **today's learning activity**:

```markdown
# Daily Log: 2026-02-15

## Session Goal

## Progress Timeline
### 14:30 â€” Started learning Machine Learning
### 14:35 â€” Mastered: Vectors
### 14:42 â€” Mastered: Mean & Median
```

**How it's used:**
- Timestamped entries are appended as the user progresses
- Reset at the start of each new day/session

### `conversation.md` â€” Chat Transcript

Raw record of the active conversation with timestamps:

```markdown
# Conversation: 2026-02-15 14:30

[14:30] user: Learn: Machine Learning
[14:30] assistant: Great! Let me break down Machine Learning...
[14:32] user: yes, makes sense
[14:32] assistant: Now let's test your understanding...
```

**How it's used:**
- Enables context-aware search across the current conversation
- Reset when a new chat session starts

### How Memory Files Connect

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     writes to      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ conversation â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  memory.md   â”‚
â”‚     .md      â”‚  (on mastery)     â”‚ (permanent)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     reads from            â”‚
â”‚  daily.md   â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ (per-session)â”‚     summarizes
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

All 3 files â”€â”€â–º search.py (queryable via hybrid search)
All 3 files â”€â”€â–º memory_manager.py (read/write/append)
```

---

## ğŸ” Hybrid Search Engine

The `search.py` module combines **two search strategies** for maximum recall:

### 1. Exact (Keyword) Search

- Splits the query into keywords (words > 2 characters)
- Scans all 3 memory files line-by-line
- Scores each line by **keyword overlap ratio** (`matched_keywords / total_keywords`)
- Returns surrounding context (2 lines above/below the match)
- Fast, high-precision â€” great for exact terms

### 2. Vector (Semantic) Search

- Uses the **`all-MiniLM-L6-v2`** sentence-transformer model to embed text
- Builds a **FAISS** (Facebook AI Similarity Search) flat L2 index over `memory.md` lines
- Query embedding is compared against the index using L2 distance
- Score is computed as `1 / (1 + distance)` â€” closer = higher score
- Embeddings are **cached in SQLite** to avoid recomputation
- The FAISS index is rebuilt only when `memory.md` content changes

### 3. Hybrid Combining

```python
search(query, top_k=8)
â”œâ”€â”€ exact_search(query, top_k=3)     # Fast keyword matches
â”œâ”€â”€ vector_search(query, top_k=5)    # Semantic similarity
â”œâ”€â”€ Deduplicate (by first 80 chars)
â”œâ”€â”€ Rank: exact matches first, then by score
â””â”€â”€ Return top_k results
```

Each result contains:
```python
{
    "file": "memory.md",       # Which file the match came from
    "line": 42,                # Line number (exact search only)
    "content": "...",          # The matching content with context
    "type": "exact|semantic",  # Which search found it
    "score": 0.85,             # Relevance score (0-1)
}
```

---

## ğŸ’¾ Caching Layer (SQLite)

The `cache.py` module uses SQLite (with WAL mode for concurrency) to persist 4 types of data:

| Table | Purpose | Key | TTL |
|---|---|---|---|
| `embedding_cache` | Sentence-transformer embeddings | SHA-256 of text | âˆ (LRU tracked) |
| `prerequisite_cache` | LLM-generated topic trees | Topic name | 7 days |
| `synthesis_cache` | Generated quiz questions | Concept + prereqs | âˆ |
| `concept_mastery` | What the user has mastered | User ID + concept | âˆ |

**Why caching matters:**
- Building a prerequisite tree requires **many LLM calls** (one per node). Caching avoids regeneration.
- Embeddings are expensive to compute. The cache stores them as pickled blobs.
- Synthesis questions can be reused if the same concept/prerequisites combination appears again.
- Mastered concepts are tracked so the learner **never re-learns** what they already know.

---

## ğŸ“š Teaching Pipeline

The teaching flow is orchestrated by `main.py` and uses 4 specialized modules:

### Step 1: Prerequisite Decomposition (`prerequisite.py`)

```
Input: "Machine Learning"
    â”‚
    â–¼
LLM Prompt: "Break down Machine Learning into 2-4 prerequisites"
    â”‚
    â–¼
Response: "1. Statistics  2. Linear Algebra  3. Optimization"
    â”‚
    â–¼
Recurse on each prerequisite (up to depth 5)
    â”‚
    â–¼
Stop when LLM says "FACT" (a concept explainable in one sentence)
    â”‚
    â–¼
Output: Tree structure with CONCEPT / FACT / LEAF nodes
```

- **Cycle detection**: Tracks visited topics to prevent infinite recursion
- **Cache**: Trees are cached for 7 days after first generation
- **Teaching order**: Post-order traversal produces a bottom-up sequence (leaves first)

### Step 2: First-Principles Explanation (`explainer.py`)

For each concept in the teaching order:
- Tells the LLM what concepts the student **already knows** (previously explained ones)
- Asks for a 5-sentence explanation a 12-year-old could understand
- Must include a **concrete real-world example** and a **"think of it like..."** analogy
- No jargon unless defined in the same sentence

### Step 3: Synthesis Question (`synthesis.py`)

After the user confirms understanding:
- Generates a **scenario-based** question combining 2-3 recent prerequisites
- The question has 2-3 sub-parts probing different angles
- Cannot be answered by **repeating definitions** â€” requires **reasoning**
- Questions are cached so the same combo doesn't regenerate

### Step 4: Answer Validation (`validator.py`)

The user's answer is scored on 4 criteria (0-100 each):
- **Coverage** â€” Did they mention all listed prerequisites?
- **Integration** â€” Did they explain how prerequisites relate?
- **Reasoning** â€” Logical reasoning, not just facts?
- **Depth** â€” Nuances, edge cases, "what if" scenarios?

**Pass threshold**: Score â‰¥ 60

| Outcome | Action |
|---|---|
| âœ… Pass | Record mastery â†’ move to next concept |
| âŒ Fail (attempts remaining) | Generate a targeted hint â†’ let them retry |
| âŒ Fail (max 3 attempts) | Explain the connections â†’ move on |

---

## ğŸš€ Setup & Installation

### Prerequisites

- **Python 3.10+** installed ([download](https://www.python.org/downloads/))
- **Git** installed ([download](https://git-scm.com/downloads))
- An **NVIDIA API key** from [build.nvidia.com](https://build.nvidia.com/) (free tier available)

### Step 1: Clone the Repository

```bash
git clone https://github.com/Adnan0042N/optimization-project.git
cd optimization-project
```

### Step 2: Create a Virtual Environment

```bash
# Windows
python -m venv venv
venv\Scripts\activate

# macOS / Linux
python3 -m venv venv
source venv/bin/activate
```

### Step 3: Install Dependencies

```bash
pip install -r backend/requirements.txt
```

This installs:
| Package | Purpose |
|---|---|
| `fastapi` | Web framework (API + static file serving) |
| `uvicorn` | ASGI server |
| `openai` | OpenAI-compatible client (used for NVIDIA API) |
| `sentence-transformers` | Local embedding model (`all-MiniLM-L6-v2`) |
| `faiss-cpu` | Vector similarity search (FAISS) |
| `numpy` | Numerical operations |
| `pydantic` | Request/response validation |
| `python-dotenv` | Environment variable loading |

> **Note:** The first run will download the `all-MiniLM-L6-v2` model (~80MB). This only happens once.

### Step 4: Configure Environment Variables

Create a `.env` file inside the `backend/` directory:

```bash
# backend/.env
NVIDIA_API_KEY=your_nvidia_api_key_here
NVIDIA_BASE_URL=https://integrate.api.nvidia.com/v1
LLM_MODEL=meta/llama-3.3-70b-instruct
EMBEDDING_MODEL=all-MiniLM-L6-v2
MEMORY_DIR=../data/memory
CACHE_DB=../data/memory/cache.db
```

**How to get your NVIDIA API key:**
1. Go to [build.nvidia.com](https://build.nvidia.com/)
2. Sign up / Log in
3. Navigate to any model â†’ click **"Get API Key"**
4. Copy the key (starts with `nvapi-`)

### Step 5: Run the Server

```bash
cd backend
python -m uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

### Step 6: Open the App

Open your browser and go to:

```
http://localhost:8000
```

You should see the LearnBot UI with a chat interface and sidebar. Type something like **"Learn: Photosynthesis"** to start!

---

## âš™ Configuration

All configuration lives in `backend/config.py` and can be overridden via `.env`:

| Variable | Default | Description |
|---|---|---|
| `NVIDIA_API_KEY` | _(required)_ | Your NVIDIA API key |
| `NVIDIA_BASE_URL` | `https://integrate.api.nvidia.com/v1` | API endpoint |
| `LLM_MODEL` | `meta/llama-3.3-70b-instruct` | LLM model to use |
| `EMBEDDING_MODEL` | `all-MiniLM-L6-v2` | Sentence-transformer model |
| `MEMORY_DIR` | `../data/memory` | Path to memory files |
| `CACHE_DB` | `../data/memory/cache.db` | Path to SQLite cache |
| `MAX_TREE_DEPTH` | `5` | Max recursion depth for prerequisite trees |
| `CACHE_TTL_DAYS` | `7` | How long cached trees remain valid |
| `SYNTHESIS_DIFFICULTY` | `medium` | Quiz difficulty (`easy` / `medium` / `hard`) |
| `SYNTHESIS_MAX_ATTEMPTS` | `3` | Max attempts before auto-advancing |
| `TOP_K_EXACT` | `3` | Max keyword search results |
| `TOP_K_VECTOR` | `5` | Max semantic search results |

---

## ğŸ“ Project Structure

```
optimization-project/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py                 # FastAPI app, routes, teaching flow orchestration
â”‚   â”œâ”€â”€ config.py               # Environment-based configuration
â”‚   â”œâ”€â”€ requirements.txt        # Python dependencies
â”‚   â”œâ”€â”€ .env                    # API keys & settings (create this yourself)
â”‚   â””â”€â”€ modules/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ llm_client.py       # NVIDIA LLM API wrapper (OpenAI-compatible)
â”‚       â”œâ”€â”€ prerequisite.py     # Recursive topic â†’ prerequisite tree builder
â”‚       â”œâ”€â”€ explainer.py        # First-principles concept explainer
â”‚       â”œâ”€â”€ synthesis.py        # Synthesis question generator
â”‚       â”œâ”€â”€ validator.py        # Answer validation, scoring, hints
â”‚       â”œâ”€â”€ search.py           # Hybrid keyword + FAISS vector search
â”‚       â”œâ”€â”€ memory_manager.py   # Read/write/append markdown memory files
â”‚       â””â”€â”€ cache.py            # SQLite caching (embeddings, trees, mastery)
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html              # Main UI (Tailwind CSS, dark theme)
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â””â”€â”€ styles.css          # Custom styles & animations
â”‚   â””â”€â”€ js/
â”‚       â”œâ”€â”€ app.js              # Main app initialization & event handlers
â”‚       â”œâ”€â”€ chat.js             # Chat message rendering & submission
â”‚       â”œâ”€â”€ tree.js             # Knowledge tree visualization
â”‚       â”œâ”€â”€ progress.js         # Progress bar & tracking
â”‚       â”œâ”€â”€ sessions.js         # Session management (localStorage)
â”‚       â””â”€â”€ memory-store.js     # Memory file sync from backend
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ memory/                 # Auto-created on first run
â”‚       â”œâ”€â”€ memory.md           # Lifetime knowledge graph
â”‚       â”œâ”€â”€ daily.md            # Today's session log
â”‚       â”œâ”€â”€ conversation.md     # Active chat transcript
â”‚       â”œâ”€â”€ cache.db            # SQLite cache database
â”‚       â””â”€â”€ state.json          # Session state snapshots
â”‚
â””â”€â”€ README.md                   # You are here
```

---

## ğŸ›  Troubleshooting

| Problem | Solution |
|---|---|
| `NVIDIA API key denied` | Make sure your key starts with `nvapi-` and is valid at [build.nvidia.com](https://build.nvidia.com/) |
| `ModuleNotFoundError` | Run `pip install -r backend/requirements.txt` inside your virtual environment |
| `Port 8000 already in use` | Use `--port 8001` or kill the existing process |
| `Model download hangs` | The first run downloads `all-MiniLM-L6-v2` (~80MB). Ensure internet access. |
| `cache.db locked` | Only one server instance should run at a time. Stop duplicates. |

---

## ğŸ“„ License

This project is for educational purposes.
